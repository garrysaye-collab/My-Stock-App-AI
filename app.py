import streamlit as st
import pandas as pd
import yfinance as yf
import numpy as np
import google.generativeai as genai
import datetime

# ==========================================
# ğŸ”§ 1. ç³»çµ±è¨­å®šèˆ‡ç‹€æ…‹åˆå§‹åŒ–
# ==========================================
st.set_page_config(page_title="Gemini 2.5 æ™ºåº«æˆ°æƒ…å®¤", page_icon="ğŸ“ˆ", layout="wide")

if "messages" not in st.session_state:
    st.session_state.messages = []
if "data_context" not in st.session_state:
    st.session_state.data_context = None

# ==========================================
# ğŸ“Š 2. é‡åŒ–æ ¸å¿ƒå‡½æ•¸ (å„ªå…ˆå®šç¾©ï¼Œè§£æ±º Import éŒ¯èª¤)
# ==========================================
def slope(series, n=3):
    """è¨ˆç®—æŒ‡æ¨™æ–œç‡"""
    y = series.tail(n).dropna()
    if len(y) < n: return 0
    return np.polyfit(np.arange(len(y)), y, 1)[0]

def detailed_scoring(df):
    """é‡åŒ–å‹•èƒ½è©•åˆ†ç³»çµ±"""
    r = df.iloc[-1]
    details = []
    total_score = 0
    
    # è©•åˆ†é‚è¼¯
    checks = [
        (r['MA5'] > r['MA10'] > r['MA20'], 3, "å‡ç·šå¤šé ­æ’åˆ—", "MA 5>10>20"),
        (slope(df['DIF']) > 0 and r['OSC'] > 0, 2, "MACD èƒ½é‡è½‰å¼·", "DIFæ–œç‡>0"),
        (r['Close'] > r['MA20'], 1, "ç«™ä¸Šæœˆç·šé—œéµä½", "Close > MA20"),
        (r['Volume'] > df['Volume'].tail(5).mean(), 1, "æˆäº¤é‡å¢æº«", "Vol > 5MA")
    ]
    
    for cond, pts, rule, desc in checks:
        s = pts if cond else 0
        details.append({"æº–å‰‡": rule, "å¾—åˆ†": s, "ç‹€æ…‹": "âœ…" if cond else "âŒ"})
        total_score += s
        
    return total_score, pd.DataFrame(details)

def comprehensive_backtest(df):
    """å›æ¸¬é‚è¼¯ (2å¹´æ•¸æ“š)"""
    log = []
    holding = False; entry_p = 0; entry_d = None
    
    # å¾ç¬¬ 20 å¤©é–‹å§‹æ¨¡æ“¬ (é ç•™ MA20 è¨ˆç®—ç©ºé–“)
    for i in range(20, len(df)):
        r = df.iloc[i]
        
        # é€²å ´æ¢ä»¶ï¼šç«™ä¸Šæœˆç·š + MACD ç´…æŸ±
        if not holding and r['Close'] > r['MA20'] and r['OSC'] > 0:
            holding = True; entry_p = r['Close']; entry_d = df.index[i]
            
        # å‡ºå ´æ¢ä»¶ï¼šè·Œç ´æœˆç·š æˆ– RSI éç†±(>85)
        elif holding and (r['Close'] < r['MA20'] or r['RSI'] > 85):
            log.append({
                "é€²å ´æ—¥æœŸ": entry_d.date(), 
                "å‡ºå ´æ—¥æœŸ": df.index[i].date(), 
                "è²·å…¥åƒ¹": round(entry_p, 2), 
                "è³£å‡ºåƒ¹": round(r['Close'], 2), 
                "ç²åˆ©%": round((r['Close']-entry_p)/entry_p*100, 2)
            })
            holding = False
            
    return pd.DataFrame(log)

def calculate_advanced_metrics(log_df):
    """è¨ˆç®—å¤æ™®å€¼èˆ‡æœ€å¤§å›æ’¤"""
    if log_df.empty: return 0, 0
    
    # å¹´åŒ–å¤æ™®å€¼ (ç°¡æ˜“ä¼°ç®—)
    returns = log_df['ç²åˆ©%'] / 100
    sharpe = (returns.mean() / returns.std() * np.sqrt(252)) if len(returns) > 1 and returns.std() != 0 else 0
    
    # æœ€å¤§å›æ’¤ (MDD)
    cum_rets = (1 + returns).cumprod()
    if cum_rets.empty: return 0, 0
    mdd = ((cum_rets - cum_rets.cummax()) / cum_rets.cummax()).min() * 100
    
    return round(sharpe, 2), round(mdd, 2)

# ==========================================
# ğŸ•¸ï¸ 3. æ•¸æ“šå¼•æ“
# ==========================================
@st.cache_data(ttl=300)
def get_data_engine(symbol):
    """æ•¸æ“šç²å–èˆ‡æŒ‡æ¨™é ç®—"""
    symbol = symbol.strip().upper()
    # ç°¡æ˜“ä»£è™Ÿè™•ç†
    if symbol.isdigit(): symbol = f"{symbol}.TW"
    elif not any(s in symbol for s in [".TW", ".TWO", ".HK", ".US", ".SS", ".SZ"]):
        if not (symbol.isalpha() and len(symbol) <= 4): symbol = f"{symbol}.TW"
    
    try:
        t = yf.Ticker(symbol)
        df = t.history(period="2y")
        if df.empty: return None, None, symbol, "æŸ¥ç„¡æ•¸æ“š"
        if isinstance(df.columns, pd.MultiIndex): df.columns = df.columns.get_level_values(0)
        
        # è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
        df['MA5'] = df['Close'].rolling(5).mean()
        df['MA10'] = df['Close'].rolling(10).mean()
        df['MA20'] = df['Close'].rolling(20).mean()
        df['DIF'] = df['Close'].ewm(span=12).mean() - df['Close'].ewm(span=26).mean()
        df['MACD'] = df['DIF'].ewm(span=9).mean()
        df['OSC'] = df['DIF'] - df['MACD']
        df['ATR'] = (df['High'] - df['Low']).rolling(14).mean()
        
        delta = df['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss.replace(0, np.nan)
        df['RSI'] = 100 - (100 / (1 + rs))
        
        return df.dropna(), t.info.get('longName', symbol), symbol, None
    except Exception as e:
        return None, None, symbol, str(e)

# ==========================================
# ğŸ¤– 4. AI æ™ºèƒ½æ ¸å¿ƒ (Gemini 2.5 Flash å„ªå…ˆ)
# ==========================================
def get_gemini_25_response(api_key, messages_history):
    genai.configure(api_key=api_key)
    
    # âœ… å„ªå…ˆç´šè¨­å®šï¼š2.5 Flash ç¬¬ä¸€
    priority_models = [
        "gemini-2.5-flash", 
        "gemini-2.0-flash", 
        "gemini-1.5-pro"
    ]
    
    # å˜—è©¦ç²å–å¯ç”¨æ¨¡å‹
    try:
        available = [m.name.replace("models/", "") for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        target_model = next((m for m in priority_models if m in available), "gemini-1.5-flash")
    except:
        target_model = "gemini-2.5-flash" # Fallback

    sys_instruction = f"""
    ç¾åœ¨æ˜¯ {datetime.datetime.now().strftime("%Y-%m-%d")}ã€‚ä½ æ˜¯é…å‚™ Gemini 2.5 Flash çš„é ‚ç´šå°æ²–åŸºé‡‘ç¶“ç†ã€‚
    
    ã€æ ¸å¿ƒä»»å‹™ã€‘
    1. **å³æ™‚è³‡è¨Š**ï¼šä½¿ç”¨ Google Search å·¥å…·æª¢ç´¢è©²å…¬å¸ã€Œä»Šæ—¥/æœ¬é€±ã€çš„æœ€æ–°è²¡å ±ã€æ³•èªªæœƒèˆ‡æ–°èã€‚
    2. **GEM åˆ†ææ¡†æ¶**ï¼š
       - **ğŸŒŠ è³‡é‡‘æµªæ½®**ï¼šåˆ†æå¤–è³‡èˆ‡ä¸»åŠ›å‹•å‘ã€‚
       - **ğŸŒ‹ çµæ§‹é¢¨éšª**ï¼šåˆ©ç”¨ç©ºæ–¹è¦–è§’æŒ‡å‡ºæ½›åœ¨å´©ç›¤é»ã€‚
       - **ğŸ­ èŠå®¶åšå¼ˆ**ï¼šæ‹†è§£å¤§æˆ¶æ´—ç›¤æˆ–èª˜å¤šé™·é˜±ã€‚
       - **ğŸ† æœ€çµ‚å‹ç®—**ï¼šç¶œåˆé‡åŒ–å¾—åˆ†ï¼Œçµ¦å‡ºæ˜ç¢ºçš„ã€ŒæŠ•è³‡å»ºè­°è©•ç­‰ã€(è²·é€²/æŒæœ‰/è³£å‡º)ã€‚
    """

    try:
        model = genai.GenerativeModel(
            model_name=target_model,
            tools=[{"google_search_retrieval": {}}],
            system_instruction=sys_instruction
        )
        
        # è½‰æ›æ­·å²æ ¼å¼
        history = []
        for m in messages_history[:-1]:
            role = "user" if m["role"] == "user" else "model"
            history.append({"role": role, "parts": [m["content"]]})
        
        chat = model.start_chat(history=history)
        response = chat.send_message(messages_history[-1]["content"])
        return response.text, target_model
    except Exception as e:
        return f"âŒ AI æœå‹™ç•°å¸¸: {str(e)}", "N/A"

# ==========================================
# ğŸ–¥ï¸ 5. UI ä»‹é¢èˆ‡ä¸»ç¨‹åº
# ==========================================
with st.sidebar:
    st.title("ğŸ¦ æ™ºåº«æ§åˆ¶ä¸­å¿ƒ")
    st.caption("Powered by Gemini 2.5 Flash")
    key = st.text_input("Gemini API Key", type="password")
    ticker = st.text_input("è‚¡ç¥¨ä»£è™Ÿ", value="2330")
    scan_btn = st.button("ğŸš€ å•Ÿå‹•æ·±åº¦æƒæ", type="primary", use_container_width=True)
    if st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±"): st.session_state.messages = []; st.rerun()

if scan_btn and key:
    with st.spinner("ğŸš€ Gemini 2.5 æ­£åœ¨èª¿é–±å…¨çƒæ•¸æ“šèˆ‡å›æ¸¬åˆ†æ..."):
        df, name, sid, err = get_data_engine(ticker)
        
        if df is not None:
            # âœ… ç›´æ¥èª¿ç”¨å·²å®šç¾©çš„å‡½æ•¸
            score, score_df = detailed_scoring(df)
            bt_log = comprehensive_backtest(df)
            sharpe, mdd = calculate_advanced_metrics(bt_log)
            
            # å­˜å…¥ Session Context
            st.session_state.data_context = {
                "df": df, "name": name, "sid": sid, "score": score, 
                "score_df": score_df, "bt_log": bt_log, "sharpe": sharpe, "mdd": mdd
            }
            
            # ç”Ÿæˆ AI Prompt
            prompt = (
                f"åˆ†æ {name} ({sid})ã€‚ç›®å‰æŠ€è¡“é¢è©•åˆ†:{score}/10, "
                f"éå»å…©å¹´è¶¨å‹¢ç­–ç•¥å›æ¸¬å¤æ™®å€¼:{sharpe}, æœ€å¤§å›æ’¤:{mdd}%ã€‚"
                f"è«‹è¯ç¶²æœå°‹ä»Šæ—¥æœ€æ–°é‡å¤§æ–°èï¼Œä¸¦é€²è¡Œå¤šç©ºè¾¯è­‰åˆ†æã€‚"
            )
            
            ai_resp, used_model = get_gemini_25_response(key, [{"role": "user", "content": prompt}])
            
            st.session_state.messages = [
                {"role": "user", "content": f"å•Ÿå‹• {sid} æ·±åº¦æƒæå ±å‘Š"},
                {"role": "assistant", "content": ai_resp}
            ]
            st.session_state.used_model = used_model
            st.rerun() # å¼·åˆ¶åˆ·æ–°ä»¥é¡¯ç¤ºçµæœ
        else:
            st.error(err)

# é¡¯ç¤ºçµæœçœ‹æ¿
if st.session_state.data_context:
    ctx = st.session_state.data_context
    st.header(f"ğŸ“Š {ctx['name']} ({ctx['sid']})")
    
    # é ‚éƒ¨é—œéµæŒ‡æ¨™
    m1, m2, m3, m4 = st.columns(4)
    m1.metric("å‹•èƒ½è©•åˆ†", f"{ctx['score']} / 10")
    m2.metric("å¤æ™®æ¯”ç‡ (Sharpe)", ctx['sharpe'])
    m3.metric("æœ€å¤§å›æ’¤ (MDD)", f"{ctx['mdd']}%")
    m4.metric("AI æ¨¡å‹", st.session_state.get('used_model', 'Pending'))

    # è‚¡åƒ¹èµ°å‹¢åœ–
    st.line_chart(ctx['df'][['Close', 'MA20']].tail(120))
    
    # è©³ç´°æ•¸æ“šæ‘ºç–Šå€
    with st.expander("ğŸ“ æŸ¥çœ‹è©³ç´°æŒ‡æ¨™è©•åˆ†èˆ‡å›æ¸¬å°å¸³å–®"):
        c_a, c_b = st.columns([1, 2])
        c_a.table(ctx['score_df'])
        if not ctx['bt_log'].empty:
            c_b.dataframe(ctx['bt_log'], use_container_width=True)
        else:
            c_b.info("éå»å…©å¹´ç„¡è§¸ç™¼é€²å ´è¨Šè™Ÿ")

    st.divider()
    
    # å°è©±å€
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # ç”¨æˆ¶è¼¸å…¥å€
    if query := st.chat_input("è©¢å• AI ç¶“ç†äººæ›´å¤šç´°ç¯€..."):
        st.session_state.messages.append({"role": "user", "content": query})
        st.chat_message("user").markdown(query)
        
        with st.chat_message("assistant"):
            with st.spinner("æ™ºåº«è¾¯è­‰ä¸­..."):
                resp, _ = get_gemini_25_response(key, st.session_state.messages)
                st.markdown(resp)
                st.session_state.messages.append({"role": "assistant", "content": resp})
